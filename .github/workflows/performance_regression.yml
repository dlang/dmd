name: Performance Regression Check
on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read
      pull-requests: write
    env:
      MODEL: 64
      BENCHMARK_REPO: https://github.com/dlang/phobos
      BENCHMARK_REF: master

    steps:
    - name: Checkout PR code
      uses: actions/checkout@v4
      with:
        path: pr-dmd

    - name: Checkout master code
      uses: actions/checkout@v4
      with:
        repository: dlang/dmd
        ref: master
        path: master-dmd

    - name: Setup D compiler
      uses: dlang-community/setup-dlang@v1.3.0
      with:
        compiler: dmd-latest

    - name: Build PR compiler
      working-directory: ./pr-dmd
      run: |
        make -j$(nproc) MODEL=$MODEL HOST_DMD=$DC
        mkdir -p generated/linux/release/$MODEL

    - name: Build master compiler
      working-directory: ./master-dmd
      run: |
        make -j$(nproc) MODEL=$MODEL HOST_DMD=$DC
        mkdir -p generated/linux/release/$MODEL

    - name: Compare compiler binaries
      id: diff_check
      run: |
        pr_compiler="./pr-dmd/generated/linux/release/$MODEL/dmd"
        master_compiler="./master-dmd/generated/linux/release/$MODEL/dmd"

        pr_md5=$(md5sum "$pr_compiler" | cut -d' ' -f1)
        master_md5=$(md5sum "$master_compiler" | cut -d' ' -f1)

        if diff -q "$pr_compiler" "$master_compiler" >/dev/null; then
          echo "DIFF_RESULT=✅ Binaries are identical (MD5: ${pr_md5:0:8})" >> $GITHUB_OUTPUT
        else
          echo "DIFF_RESULT=❌ Binaries differ (PR MD5: ${pr_md5:0:8}, Master MD5: ${master_md5:0:8})" >> $GITHUB_OUTPUT
        fi

    - name: Checkout benchmark project
      uses: actions/checkout@v4
      with:
        repository: dlang/phobos
        ref: ${{ env.BENCHMARK_REF }}
        path: benchmark-phobos

    - name: Install hyperfine
      run: sudo apt-get install -y hyperfine jq

    - name: Warmup build cache
      working-directory: ./benchmark-phobos
      run: |
        ../pr-dmd/generated/linux/release/$MODEL/dmd -i=std -c -unittest -version=StdUnittest -preview=dip1000 std/package.d || true
        rm -f *.o

    - name: Run benchmarks
      working-directory: ./benchmark-phobos
      run: |
        hyperfine \
          --warmup 1 \
          --runs 5 \
          --show-output \
          --export-json benchmark_results.json \
          --prepare 'rm -f *.o' \
          "../pr-dmd/generated/linux/release/$MODEL/dmd -i=std -c -unittest -version=StdUnittest -preview=dip1000 std/package.d" \
          "../master-dmd/generated/linux/release/$MODEL/dmd -i=std -c -unittest -version=StdUnittest -preview=dip1000 std/package.d"

    - name: Parse results
      id: results
      run: |
        if [ ! -f benchmark-phobos/benchmark_results.json ]; then
          echo "::error::Benchmark results file not found!"
          exit 1
        fi

        # Validate JSON structure
        if ! jq -e '.results | length >= 2' benchmark-phobos/benchmark_results.json >/dev/null; then
          echo "::error::Invalid benchmark results format"
          exit 1
        fi

        # Extract raw data with error handling
        pr_times=($(jq -r '.results[0].times[]? | select(. != null) | tonumber' benchmark-phobos/benchmark_results.json || true))
        pr_mem=($(jq -r '.results[0].max_rss[]? | select(. != null) | tonumber' benchmark-phobos/benchmark_results.json || true))
        master_times=($(jq -r '.results[1].times[]? | select(. != null) | tonumber' benchmark-phobos/benchmark_results.json || true))
        master_mem=($(jq -r '.results[1].max_rss[]? | select(. != null) | tonumber' benchmark-phobos/benchmark_results.json || true))

        if [ ${#pr_times[@]} -eq 0 ] || [ ${#master_times[@]} -eq 0 ]; then
          echo "::error::Missing time measurements in benchmark results"
          exit 1
        fi

        # Format individual runs
        pr_time_runs=$(printf "%.3f, " "${pr_times[@]}" | sed 's/, $//')
        pr_mem_runs=$(for m in "${pr_mem[@]}"; do printf "%.1f MB, " "$(echo "$m/1024" | bc -l)"; done | sed 's/, $//')
        master_time_runs=$(printf "%.3f, " "${master_times[@]}" | sed 's/, $//')
        master_mem_runs=$(for m in "${master_mem[@]}"; do printf "%.1f MB, " "$(echo "$m/1024" | bc -l)"; done | sed 's/, $//')

        # Calculate averages
        pr_time_avg=$(jq -r '.results[0].mean // "NaN"' benchmark-phobos/benchmark_results.json | awk '{printf "%.3f", $1}')
        pr_time_stddev=$(jq -r '.results[0].stddev // "NaN"' benchmark-phobos/benchmark_results.json | awk '{printf "%.3f", $1}')
        master_time_avg=$(jq -r '.results[1].mean // "NaN"' benchmark-phobos/benchmark_results.json | awk '{printf "%.3f", $1}')
        master_time_stddev=$(jq -r '.results[1].stddev // "NaN"' benchmark-phobos/benchmark_results.json | awk '{printf "%.3f", $1}')

        # Set outputs
        echo "pr_time_runs=${pr_time_runs}" >> $GITHUB_OUTPUT
        echo "pr_mem_runs=${pr_mem_runs}" >> $GITHUB_OUTPUT
        echo "master_time_runs=${master_time_runs}" >> $GITHUB_OUTPUT
        echo "master_mem_runs=${master_mem_runs}" >> $GITHUB_OUTPUT
        echo "pr_time_avg=${pr_time_avg} ± ${pr_time_stddev}" >> $GITHUB_OUTPUT
        echo "master_time_avg=${master_time_avg} ± ${master_time_stddev}" >> $GITHUB_OUTPUT
        echo "binary_diff=${{ steps.diff_check.outputs.DIFF_RESULT }}" >> $GITHUB_OUTPUT

    - name: Create comment
      run: |
        PERFORMANCE_COMMENT=$(cat << EOF
        ### 🚀 Performance Benchmark Results

        **Individual Runs:**
        | Run | PR Time (s) | Master Time (s) | PR Memory (MB) | Master Memory (MB) |
        |-----|-------------|-----------------|----------------|---------------------|
        $(
          pr_times=(${pr_time_runs//, / })
          master_times=(${master_time_runs//, / })
          pr_mems=(${pr_mem_runs//, / })
          master_mems=(${master_mem_runs//, / })
          for i in {0..4}; do
            printf "| %d | ${pr_times[$i]:-N/A} | ${master_times[$i]:-N/A} | ${pr_mems[$i]:-N/A} | ${master_mems[$i]:-N/A} |\\n" $((i+1))
          done
        )

        **Summary Statistics:**
        | Metric | PR | Master |
        |--------|----|--------|
        | ⏱️ Time (s) | ${{ steps.results.outputs.pr_time_avg }} | ${{ steps.results.outputs.master_time_avg }} |

        **Binary Comparison:**
        ${{ steps.results.outputs.binary_diff }}

        <details>
        <summary>📊 Additional Details</summary>

        - Benchmark target: Phobos@${{ env.BENCHMARK_REF }}
        - Runs: 5 (after 1 warmup)
        - Hyperfine version: \`$(hyperfine --version)\`
        - Executed on: $(date -u +'%Y-%m-%d %H:%M:%S UTC')
        </details>
        EOF
        )
        echo "$PERFORMANCE_COMMENT" > performance_comment.md

    - name: Post comment
      uses: actions/github-script@v6
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const fs = require('fs');
          try {
            const comment = fs.readFileSync('performance_comment.md', 'utf8');
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const existingComment = comments.find(c => c.body.includes('🚀 Performance Benchmark Results'));

            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: comment,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment,
              });
            }
          } catch (error) {
            core.error('Failed to post comment: ' + error.message);
          }
