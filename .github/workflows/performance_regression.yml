name: Performance Regression Check
on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read
      pull-requests: write
    env:
      MODEL: 64
      BENCHMARK_REPO: https://github.com/dlang/phobos
      BENCHMARK_REF: master

    steps:
    - name: Checkout PR code
      uses: actions/checkout@v4
      with:
        path: pr-dmd

    - name: Checkout master code
      uses: actions/checkout@v4
      with:
        repository: dlang/dmd
        ref: master
        path: master-dmd

    - name: Setup D compiler
      uses: dlang-community/setup-dlang@v1.3.0
      with:
        compiler: dmd-latest

    - name: Build PR compiler
      working-directory: ./pr-dmd
      run: |
        make -j$(nproc) MODEL=$MODEL HOST_DMD=$DC
        mkdir -p generated/linux/release/$MODEL

    - name: Build master compiler
      working-directory: ./master-dmd
      run: |
        make -j$(nproc) MODEL=$MODEL HOST_DMD=$DC
        mkdir -p generated/linux/release/$MODEL

    - name: Compare compiler binaries
      id: diff_check
      run: |
        pr_compiler="./pr-dmd/generated/linux/release/$MODEL/dmd"
        master_compiler="./master-dmd/generated/linux/release/$MODEL/dmd"

        pr_md5=$(md5sum "$pr_compiler" | cut -d' ' -f1)
        master_md5=$(md5sum "$master_compiler" | cut -d' ' -f1)

        if diff -q "$pr_compiler" "$master_compiler" >/dev/null; then
          echo "DIFF_RESULT=âœ… Binaries are identical (MD5: ${pr_md5:0:8})" >> $GITHUB_OUTPUT
        else
          echo "DIFF_RESULT=âŒ Binaries differ (PR MD5: ${pr_md5:0:8}, Master MD5: ${master_md5:0:8})" >> $GITHUB_OUTPUT
        fi

    - name: Checkout benchmark project
      uses: actions/checkout@v4
      with:
        repository: dlang/phobos
        ref: ${{ env.BENCHMARK_REF }}
        path: benchmark-phobos

    - name: Install hyperfine
      run: sudo apt-get install -y hyperfine jq

    - name: Warmup build cache
      working-directory: ./benchmark-phobos
      run: |
        ../pr-dmd/generated/linux/release/$MODEL/dmd -i=std -c -unittest -version=StdUnittest -preview=dip1000 std/package.d || true
        rm -f *.o

    - name: Run benchmarks
      working-directory: ./benchmark-phobos
      run: |
        hyperfine \
          --warmup 1 \
          --runs 5 \
          --show-output \
          --export-json benchmark_results.json \
          --prepare 'rm -f *.o' \
          "../pr-dmd/generated/linux/release/$MODEL/dmd -i=std -c -unittest -version=StdUnittest -preview=dip1000 std/package.d" \
          "../master-dmd/generated/linux/release/$MODEL/dmd -i=std -c -unittest -version=StdUnittest -preview=dip1000 std/package.d"

    - name: Parse results
      id: results
      run: |
        if [ ! -f benchmark-phobos/benchmark_results.json ]; then
          echo "Error: Benchmark results file not found!"
          exit 1
        fi

        # Extract raw data
        pr_times=($(jq -r '.results[0].times[] | tonumber | @text' benchmark-phobos/benchmark_results.json))
        pr_mem=($(jq -r '.results[0].max_rss[] | tonumber | @text' benchmark-phobos/benchmark_results.json))
        master_times=($(jq -r '.results[1].times[] | tonumber | @text' benchmark-phobos/benchmark_results.json))
        master_mem=($(jq -r '.results[1].max_rss[] | tonumber | @text' benchmark-phobos/benchmark_results.json))

        # Format individual runs
        pr_time_runs=$(printf "%.3f, " "${pr_times[@]}" | sed 's/, $//')
        pr_mem_runs=$(printf "%.1f MB, " "${pr_mem[@]/%//1024}" | bc -l | sed 's/, $//')
        master_time_runs=$(printf "%.3f, " "${master_times[@]}" | sed 's/, $//')
        master_mem_runs=$(printf "%.1f MB, " "${master_mem[@]/%//1024}" | bc -l | sed 's/, $//')

        # Calculate averages
        pr_time_avg=$(jq -r '.results[0].mean' benchmark-phobos/benchmark_results.json | awk '{printf "%.3f", $1}')
        pr_time_stddev=$(jq -r '.results[0].stddev' benchmark-phobos/benchmark_results.json | awk '{printf "%.3f", $1}')
        master_time_avg=$(jq -r '.results[1].mean' benchmark-phobos/benchmark_results.json | awk '{printf "%.3f", $1}')
        master_time_stddev=$(jq -r '.results[1].stddev' benchmark-phobos/benchmark_results.json | awk '{printf "%.3f", $1}')

        pr_mem_avg=$(echo "${pr_mem[@]}" | tr ' ' '\n' | awk '{sum += $1} END {printf "%.1f", sum/NR/1024}')
        pr_mem_stddev=$(echo "${pr_mem[@]}" | tr ' ' '\n' | awk '{sum += $1; sumsq += ($1)^2} END {printf "%.1f", sqrt(sumsq/NR - (sum/NR)^2)/1024}')
        master_mem_avg=$(echo "${master_mem[@]}" | tr ' ' '\n' | awk '{sum += $1} END {printf "%.1f", sum/NR/1024}')
        master_mem_stddev=$(echo "${master_mem[@]}" | tr ' ' '\n' | awk '{sum += $1; sumsq += ($1)^2} END {printf "%.1f", sqrt(sumsq/NR - (sum/NR)^2)/1024}')

        # Set outputs
        echo "pr_time_runs=${pr_time_runs}" >> $GITHUB_OUTPUT
        echo "pr_mem_runs=${pr_mem_runs}" >> $GITHUB_OUTPUT
        echo "master_time_runs=${master_time_runs}" >> $GITHUB_OUTPUT
        echo "master_mem_runs=${master_mem_runs}" >> $GITHUB_OUTPUT
        echo "pr_time_avg=${pr_time_avg} Â± ${pr_time_stddev}" >> $GITHUB_OUTPUT
        echo "master_time_avg=${master_time_avg} Â± ${master_time_stddev}" >> $GITHUB_OUTPUT
        echo "pr_mem_avg=${pr_mem_avg} Â± ${pr_mem_stddev}" >> $GITHUB_OUTPUT
        echo "master_mem_avg=${master_mem_avg} Â± ${master_mem_stddev}" >> $GITHUB_OUTPUT
        echo "binary_diff=${{ steps.diff_check.outputs.DIFF_RESULT }}" >> $GITHUB_OUTPUT

    - name: Create performance comment
      uses: actions/github-script@v6
      with:
        github-token: ${{ secrets.GITHUB_TOKEN }}
        script: |
          const formatRun = (runs, index) => {
            const values = runs.split(', ');
            return values[index] || 'N/A';
          };

          const prTimes = process.env.PR_TIME_RUNS.split(', ');
          const prMem = process.env.PR_MEM_RUNS.split(', ');
          const masterTimes = process.env.MASTER_TIME_RUNS.split(', ');
          const masterMem = process.env.MASTER_MEM_RUNS.split(', ');

          let runTables = '';
          for (let i = 0; i < 5; i++) {
            runTables += `
            | ${i+1} | ${formatRun(process.env.PR_TIME_RUNS, i)} | ${formatRun(process.env.MASTER_TIME_RUNS, i)} | ${formatRun(process.env.PR_MEM_RUNS, i)} | ${formatRun(process.env.MASTER_MEM_RUNS, i)} |`;
          }

          const output = `### ðŸš€ Performance Benchmark Results

          **Individual Runs:**
          | Run | PR Time (s) | Master Time (s) | PR Memory (MB) | Master Memory (MB) |
          |-----|-------------|-----------------|----------------|---------------------|${runTables}

          **Summary Statistics:**
          | Metric | PR | Master |
          |--------|----|--------|
          | â±ï¸ Time (s) | ${process.env.PR_TIME_AVG} | ${process.env.MASTER_TIME_AVG} |
          | ðŸ§  Memory (MB) | ${process.env.PR_MEM_AVG} | ${process.env.MASTER_MEM_AVG} |

          **Binary Comparison:**
          ${process.env.BINARY_DIFF}

          <details>
          <summary>ðŸ“Š Additional Details</summary>

          - Benchmark target: Phobos@${process.env.BENCHMARK_REF}
          - Runs: 5 (after 1 warmup)
          - Hyperfine version: \`$(hyperfine --version)\`
          - Executed on: ${new Date().toUTCString()}
          </details>`;

          github.rest.issues.createComment({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
            body: output
          });
